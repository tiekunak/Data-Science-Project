{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip intall all needed libraries\n",
    "pip install lxml\n",
    "pip install tensorflow\n",
    "pip install transformers\n",
    "pip install awswrangler\n",
    "pip install wordcloud\n",
    "pip install nltk\n",
    "\n",
    "# import nltk (natural language tool kit), a popular python package for text mining\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from os import path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# import nltk (natural language tool kit), a popular python package for text mining\n",
    "import nltk\n",
    "# stopwords, FreqDist, word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords = stopwords.words('english')\n",
    "import matplotlib.pyplot as plt\n",
    "# sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Get data from s3\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('bucket_name')\n",
    "prefix_objs = bucket.objects.filter(Prefix=\"data_name\")\n",
    "\n",
    "prefix_df = []\n",
    "\n",
    "for obj in prefix_objs:\n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    temp = pd.read_parquet(io.BytesIO(body))        \n",
    "    prefix_df.append(temp)\n",
    "df = pd.concat(prefix_df)\n",
    "df.head()\n",
    "\n",
    "#set the desired column to list\n",
    "data = df.title.tolist()\n",
    "data\n",
    "\n",
    "#Clean data for simple word count\n",
    "import lxml \n",
    "from html import unescape\n",
    "import re\n",
    "from bs4 import BeautifulSoup   # python package to handle HTML\n",
    "\n",
    "def freq_analysis(texts):\n",
    "    tokens = str(texts)\n",
    "    #lowecases\n",
    "    tokens = tokens.lower()\n",
    "    # remove HTML related characters (&amp’,’&quot’,etc.)\n",
    "    #soup = BeautifulSoup(unescape(tokens), 'lxml')\n",
    "    # remove urls\n",
    "    tokens = re.sub(r'http\\S+', '', tokens)\n",
    "    # Remove useless numbers and alphanumerical words\n",
    "    tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "    #tokenization or word split\n",
    "    tokens = word_tokenize(tokens)\n",
    "    # Filter non-alphanumeric characters from tokens\n",
    "    tokens = (word for word in tokens if word.isalpha())\n",
    "    #additional stopwrods\n",
    "    more_stopwords = set(('cant', 'aint', 'today'))\n",
    "    #extra_stoplist = set(stopwords.words('english')) | more_stopwords\n",
    "    #tokens = (word for word in tokens if word not in extra_stoplist)\n",
    "    #remove short words\n",
    "    tokens = (word for word in tokens if len(word) >= 3)\n",
    "    #compute frequency distribution for all the bigrams in the text\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    \n",
    "    #k refers to keys (or tokens); v refers to values (or counts)\n",
    "    return  fdist\n",
    "\n",
    "# identifying first 10 words\n",
    "fdist = freq_analysis(data)\n",
    "for k,v in list(fdist.items())[:10]:\n",
    "    print(k,v)\n",
    "\n",
    "def clean_data(texts):\n",
    "    tokens = str(texts)\n",
    "    #lowecases\n",
    "    tokens = tokens.lower()\n",
    "    # remove HTML related characters (&amp’,’&quot’,etc.)\n",
    "    #soup = BeautifulSoup(unescape(tokens), 'lxml')\n",
    "    # remove urls\n",
    "    tokens = re.sub(r'http\\S+', '', tokens)\n",
    "    # Remove useless numbers and alphanumerical words\n",
    "    tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "    #tokenization or word split\n",
    "    tokens = word_tokenize(tokens)\n",
    "    # Filter non-alphanumeric characters from tokens\n",
    "    tokens = (word for word in tokens if word.isalpha())\n",
    "    #additional stopwrods\n",
    "    more_stopwords = set(('cant', 'aint', 'today'))\n",
    "    #extra_stoplist = set(stopwords.words('english')) | more_stopwords\n",
    "    #tokens = (word for word in tokens if word not in extra_stoplist)\n",
    "    #remove short words\n",
    "    tokens = (word for word in tokens if len(word) >= 3)\n",
    "    \n",
    "    #k refers to keys (or tokens); v refers to values (or counts)\n",
    "    return tokens\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "#Performing Wordcloud\n",
    "\n",
    "#mask = np.array(Image.open(\"data/butterfly.png\"))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"said\")\n",
    "stopwords.add(\"the\")\n",
    "stopwords.add(\"you\")\n",
    "stopwords.add(\"yours\")\n",
    "stopwords.add(\"can\")\n",
    "stopwords.add(\"and\")\n",
    "stopwords.add(\"that\")\n",
    "stopwords.add(\"will\")\n",
    "stopwords.add(\"one\")\n",
    "stopwords.add(\"able\")\n",
    "stopwords.add(\"etc\")\n",
    "stopwords.add(\"has\")\n",
    "stopwords.add(\"done\")\n",
    "stopwords.add(\"see\")\n",
    "stopwords.add(\"new\")\n",
    "stopwords.add(\"first\")\n",
    "\n",
    "wordcloud = WordCloud(max_words=1000, stopwords=stopwords, margin=10,\n",
    "               random_state=1).generate(' '.join(data))\n",
    "\n",
    "#Plotting wordcloud\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Set the desired column for sentitment analysis. Here we are using the title column\n",
    "data = df.title.tolist()\n",
    "data\n",
    "\n",
    "# What is the overal sentiment? How many positive? how many negative? \n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "finaldata = []\n",
    "\n",
    "for i in data: \n",
    "    vs = analyzer.polarity_scores(i)\n",
    "    finaldata.append([i, vs[\"compound\"], vs['neg'], vs['neu'], vs['pos']])\n",
    "\n",
    "len(finaldata)\n",
    "\n",
    "# obtaining a dataframe for the text data.\n",
    "df_2 = pd.DataFrame(finaldata)\n",
    "df_2.columns = ['comments', 'compound', 'negative', 'neutral', 'positive']\n",
    "df_2.head()\n",
    "\n",
    "# Finding out the number of positive, negative and neutal words\n",
    "positive_reviews = df_2.loc[df_2['compound'] > 0] # if compoundscore is > 0 then positive\n",
    "negative_reviews = df_2.loc[df_2['compound'] < 0] # if compoundscore is < 0 then negative\n",
    "neutral_reviews = df_2.loc[df_2['compound'] == 0] # if compoundscore is == 0 then neutral\n",
    "\n",
    "print(len(positive_reviews))\n",
    "print(len(negative_reviews))\n",
    "print(len(neutral_reviews))\n",
    "\n",
    "#Merging the sentiment dataframe with the original dataframe\n",
    "df_2 = pd.concat([df, df_2.reindex(df.index)], axis=1)\n",
    "df_2 = df_2[['place_id', 'comments', 'compound', 'negative', 'neutral', 'positive']]\n",
    "df_2.rename(columns={'place_id': 'id'}, inplace=True)\n",
    "\n",
    "# Changing all float data type to decimal since dynamoDB does not take in numbers\n",
    "from decimal import Decimal, Context\n",
    "ctx = Context(prec=5)\n",
    "df_2.compound = df_2['compound'].apply(lambda x: ctx.create_decimal_from_float(x))\n",
    "df_2.positive = df_2['positive'].apply(lambda x: ctx.create_decimal_from_float(x))\n",
    "df_2.neutral = df_2['neutral'].apply(lambda x: ctx.create_decimal_from_float(x))\n",
    "df_2.negative = df_2['negative'].apply(lambda x: ctx.create_decimal_from_float(x))\n",
    "#df_2.compound = df_2.compound.apply(lambda x: Decimal(x))\n",
    "df_2.id = df_2.id.astype(str)\n",
    "df_2.head()\n",
    "\n",
    "#Sentiment Analysis with HuggingFace\n",
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "#data = [\"I love you\", \"I hate you\"]\n",
    "results = sentiment_pipeline(data)\n",
    "\n",
    "#Displaying first 10\n",
    "results[:10]\n",
    "\n",
    "#Put results to a dataframe\n",
    "df_3 = pd.DataFrame(results)\n",
    "df_3.head()\n",
    "\n",
    "#Merging Sentiment scores with original dataframe\n",
    "df_3 = pd.concat([df, df_3.reindex(df.index)], axis=1)\n",
    "df_3 = df_3[['place_id', 'title', 'label', 'score']]\n",
    "df_3.rename(columns={'place_id': 'id'}, inplace=True)\n",
    "df_3.head()\n",
    "\n",
    "#changing float to decimal\n",
    "from decimal import Decimal, Context\n",
    "ctx = Context(prec=5)\n",
    "df_3.score = df_3['score'].apply(lambda x: ctx.create_decimal_from_float(x))\n",
    "#df_2.compound = df_2.compound.apply(lambda x: Decimal(x))\n",
    "df_3.id = df_3.id.astype(str)\n",
    "df_3.head()\n",
    "\n",
    "#Loading results to dynamoDB\n",
    "import awswrangler as wr\n",
    "wr.dynamodb.put_df(df=df_2, table_name=\"table_name\")\n",
    "wr.dynamodb.put_df(df=df_3, table_name=\"table_name\")\n",
    "\n",
    "#Delete objects in the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket('bucket_name')\n",
    "bucket_versioning = s3.BucketVersioning('bucket_name')\n",
    "if bucket_versioning.status == 'Enabled':\n",
    "    s3_bucket.object_versions.delete()\n",
    "else:\n",
    "    s3_bucket.objects.all().delete()\n",
    "\n",
    "\n",
    "#shutting down notebook instance\n",
    "import time\n",
    "# using sleep() to hault the code execution\n",
    "time.sleep(600)\n",
    "print('Closing idle notebook')\n",
    "client = boto3.client('sagemaker')\n",
    "client.stop_notebook_instance(NotebookInstanceName= \"notebook_instance_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
